# Introduction

**Goal:** To create a model of linear logic gates in Python using perceptron algorithm and NumPy and compare its behaviour to that of a similar model created previously.

**Purpose:** To demonstrate the problem-solving and decision-making processes, as well as enhance the understanding of neural networks’ most basic components and their implementation in Python.

## Previous Model (perceptron 1)
The model is based on the mathematical description of the perceptron from Artificial Neural Networks (ANNs) course. Built without Large Language Model (LLM) assistance.
- [Code](url-to-code)

## New Model (perceptron 2)
GPT-4 was used to generate bulk of the code to experiment with LLM assistance.
- [Code](url-to-code)

## Logic gates (AND, OR)

A logic gate is a Boolean function performed on a set of binary inputs to produce a single binary output:

| A | B | AND | OR |
|---|---|-----|----|
| 0 | 0 |  0  | 0  |
| 0 | 1 |     | 1  |
| 1 | 0 |     |    |
| 1 | 1 |  1  |    |

AND, OR logic gates can be modelled by plotting a single straight line (‘hyperplane’ or ‘decision boundary’) and classifying inputs lying on one side as AND, OR gate respectively.

## Theoretical Principles Behind Perceptron
Out of scope (see [Wiki](url-to-wiki)).

## Perceptron 2

### Initial Model Development

**Steps:**
1. Initial algorithm generated by GPT-4
2. Algorithm tested for code and prediction errors – no errors encountered
3. Detailed algorithm explanation by GPT-4 and follow up questions until every block’s purpose was understood
4. Early stoppage added using GPT-4 as the algorithm was bound to converge due to linear data separability (see the graphs above)
5. Decision boundary plot added using GPT-4

### Initial Model Analysis

The decision boundary plot surfaced that:
- The final weights (and therefore the decision boundary) differed from perceptron 1
- Positive class lied on the decision boundary (unlike with perceptron 1)
- Adjusting learning rate only affected the weight vector magnitude (and not the decision boundary)

### Perceptron 2 & 1 Weight Difference Analysis

Potential factors contributing to weight differences between Perceptron 1 & 2 were assessed and ruled out:
- Sample order
- Initial weight/bias
- Learning rate

(For the complete analysis and tables, please refer to the original text)

## Perceptron 2 Activation Function Update

Adding a condition to the error function seemed like an inelegant solution, introducing complexity and preventing the calculation of residuals with a single mathematical expression.

Previously, GPT-4 had advised that in the case of binary probabilistic activation functions (such as sigmoid activation function) the points on the decision boundary could output 0.5 (50%).

This concept seemed appropriate for Perceptron’s 2 activation function as well, with a boundary point producing an output of 0.5, situated between 0 and 1. Residuals in the range (-0.5, 0.5) preserved the 'direction' of the required update, and the magnitude of the update was scaled by 0.5, reflecting that points on the decision boundary would typically require smaller updates for correct classification.

The algorithm implementation on Wiki had an activation function identical to Perceptron’s 2, i.e., `return np.where(x>0, 1, 0)`. But further examination of the unit step function used as the activation function showed that it could yield either 0, 0.5, or 1 for an input of 0, depending on interpretation. This flexibility meant we could extend the activation function output without changing the function type itself.

The initial attempt to include the additional condition by rewriting the `np.where(x>0, 1, 0)` function in an if/elif/else format resulted in an error when the 'predict' method was invoked from the Perceptron class instance. This issue arose because while the 'fit' method passed a float to the activation function, 'predict' passed an array (all samples at once).

The simplest solution seemed to add additional condition to the existing `np.where` function. GPT-4 customised the existing `return np.where(x>0, 1, 0)` by introducing a nested condition, thus resolving the compatibility issue.

The updated function was:

```python
return np.where(x < 0, 0, 
                np.where(x == 0, 0.5, 1))